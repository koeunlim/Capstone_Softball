{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "productive-remedy",
   "metadata": {},
   "source": [
    "## [NYCDSA Capstone Project] \n",
    "# Women's Softball League Power Ranking Estimate\n",
    "\n",
    "<br>\n",
    "Koeun Lim (koeunlim@alum.mit.edu)<br>\n",
    "Kevin Haghi (kevin.haghi@gmail.com)<br>\n",
    "\n",
    "\n",
    "# Step 6-1. Modeling 2 - WS 64 Seed - XGBoost Classification\n",
    "\n",
    "---\n",
    "## Project Description\n",
    "\n",
    "\n",
    "\n",
    "### Project Outline\n",
    "- Step 1. Web scraping\n",
    "- Step 2. Clean data\n",
    "- Step 3. EDA\n",
    "- Step 4. Imputation & PCA\n",
    "- Step 5-1. Modeling (1) RPI prediction based on stats - XGboost regression\n",
    "- Step 5-2. Modeling (1) RPI prediction based on stats - Linear regression\n",
    "- Step 6-1. Modeling (2) WS 64 Seed prediction based on stats - XGboost classification\n",
    "- Step 6-2. Modeling (2) WS 64 Seed prediction based on stats - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "declared-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "pd.set_option('display.max_columns', 250)\n",
    "pd.set_option('display.max_rows', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limiting-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_WSin = pd.read_csv('Data/y_WSin.csv')\n",
    "y_WSin_2019 = pd.read_csv('Data/y_WSin_2019.csv')\n",
    "\n",
    "X_famd = pd.read_csv('Data/X_famd.csv')\n",
    "X_famd_2019 = pd.read_csv('Data/X_famd_2019.csv')\n",
    "X_College = pd.read_csv('Data/X_College.csv')\n",
    "X_College_2019 = pd.read_csv('Data/X_College_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "passing-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "israeli-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "    'min_child_weight': [1, 2, 5, 10],\n",
    "    'gamma': [0, 1, 2, 5],\n",
    "    'reg_alpha': [0],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'random_state' :[1]\n",
    "}\n",
    "\n",
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic',nthread= -1,gpu_id=-1,base_score = 1387/1771)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reverse-hospital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[17:38:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      " Time taken: 0 hours 6 minutes and 7.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "folds = 5\n",
    "param_comb = 100\n",
    "\n",
    "random_search1 = RandomizedSearchCV(\n",
    "    xg_clf, \n",
    "    param_distributions=params, \n",
    "    n_iter=param_comb, \n",
    "    scoring='accuracy',\n",
    "    eval_metric='log loss',\n",
    "    n_jobs=-1, \n",
    "    cv=folds, \n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search1.fit(X_famd,y_WSin)\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "recreational-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([0.94701452, 4.06164346, 4.94097033, 1.63047724, 0.89454222,\n",
      "       4.64315734, 3.29819341, 1.87403269, 5.52353935, 1.63228655,\n",
      "       1.03002057, 3.56197033, 1.4184072 , 3.9451829 , 2.82338524,\n",
      "       2.63278255, 4.84574842, 4.17843151, 4.56877956, 1.52846146,\n",
      "       4.81354089, 3.31990266, 0.64860144, 1.10033736, 2.8635385 ,\n",
      "       2.7308722 , 5.13618808, 3.20485315, 3.83571129, 3.37432113,\n",
      "       0.91449242, 3.72556891, 3.36594486, 3.71045604, 3.46633081,\n",
      "       1.31937857, 4.52916489, 4.56435523, 1.81921215, 1.95802312,\n",
      "       4.90093122, 2.08028235, 1.28137302, 2.94773641, 0.95988898,\n",
      "       3.19723468, 2.47627687, 1.25166345, 1.69949203, 0.98852863,\n",
      "       0.80451632, 1.1631845 , 1.1650898 , 1.822087  , 3.53176899,\n",
      "       4.01750655, 3.58242359, 4.62520156, 2.39305453, 6.94220467,\n",
      "       1.70277939, 1.7088666 , 5.84579735, 0.99250631, 2.81868792,\n",
      "       4.78119617, 1.59474435, 3.0703104 , 0.89738822, 2.46415453,\n",
      "       2.54262309, 2.91335778, 2.25968838, 5.1328177 , 1.2423676 ,\n",
      "       3.94269719, 4.34440508, 3.70347776, 4.39328218, 2.70858092,\n",
      "       1.3137887 , 1.99585295, 3.43995895, 2.22343445, 3.23209805,\n",
      "       3.14900451, 2.22334604, 2.52705145, 1.95473351, 1.310571  ,\n",
      "       0.97828012, 3.03813725, 2.34586415, 4.64955845, 1.70158205,\n",
      "       3.32314177, 5.17606077, 3.32856851, 5.78079796, 3.17169456]), 'std_fit_time': array([0.01242254, 0.59859643, 0.10841984, 0.33602552, 0.04457622,\n",
      "       0.27645429, 0.09296712, 0.11262523, 0.129084  , 0.04176995,\n",
      "       0.04977545, 0.22401269, 0.04499088, 0.03897867, 0.12976717,\n",
      "       0.19836777, 0.28290254, 0.5959915 , 0.28886252, 0.21887395,\n",
      "       0.64419671, 0.3254173 , 0.06386596, 0.02826998, 0.29976749,\n",
      "       0.40466396, 0.1267439 , 0.15066103, 0.26062199, 0.36347431,\n",
      "       0.0797223 , 0.12163469, 0.185828  , 0.03526471, 0.42753041,\n",
      "       0.07575801, 0.22317547, 0.08094348, 0.07464418, 0.16127311,\n",
      "       0.32025509, 0.07228775, 0.14482749, 0.10246462, 0.02783647,\n",
      "       0.09179492, 0.14009469, 0.18570788, 0.32669754, 0.12045357,\n",
      "       0.03164833, 0.02644022, 0.02699437, 0.10598297, 0.15862068,\n",
      "       0.26137274, 0.40677062, 0.38696816, 0.08260394, 0.97010675,\n",
      "       0.29960833, 0.1906836 , 0.92376634, 0.03050963, 0.14334218,\n",
      "       0.441008  , 0.07591109, 0.2739621 , 0.03468025, 0.06061099,\n",
      "       0.06111587, 0.47303022, 0.58577153, 0.75480186, 0.09485559,\n",
      "       1.11847887, 0.95392537, 0.06045544, 0.15471592, 0.11918197,\n",
      "       0.03123563, 0.07688883, 0.16848732, 0.12352856, 0.20584827,\n",
      "       0.21125378, 0.03272641, 0.12430864, 0.06348059, 0.04908787,\n",
      "       0.04432981, 0.50439677, 0.35889219, 0.14484715, 0.1432015 ,\n",
      "       0.21639715, 0.32335099, 0.25290777, 0.20081557, 0.77361069]), 'mean_score_time': array([0.01097045, 0.00762515, 0.00612631, 0.00865541, 0.00625296,\n",
      "       0.00763283, 0.00824323, 0.00867076, 0.01128426, 0.00644484,\n",
      "       0.00656381, 0.00686073, 0.00613565, 0.00830145, 0.01147175,\n",
      "       0.00604029, 0.03975439, 0.01228085, 0.0072051 , 0.00955901,\n",
      "       0.00954342, 0.00777888, 0.00573535, 0.006601  , 0.02292805,\n",
      "       0.00679221, 0.01044607, 0.00760889, 0.00789809, 0.00639658,\n",
      "       0.00673461, 0.00714002, 0.00632524, 0.00751667, 0.00630727,\n",
      "       0.00792079, 0.00836735, 0.00815406, 0.0082509 , 0.00684805,\n",
      "       0.00685844, 0.00686173, 0.00740762, 0.00704737, 0.00705729,\n",
      "       0.00774055, 0.00570545, 0.01218462, 0.00672379, 0.00581861,\n",
      "       0.00652366, 0.00630641, 0.00624032, 0.00673261, 0.00766926,\n",
      "       0.00875821, 0.00896606, 0.00701318, 0.00722852, 0.01168909,\n",
      "       0.00933766, 0.02288499, 0.01466084, 0.00679131, 0.00825968,\n",
      "       0.00699625, 0.00691371, 0.00741577, 0.00759959, 0.00687037,\n",
      "       0.00775499, 0.00624957, 0.04652181, 0.01014752, 0.00850315,\n",
      "       0.01126256, 0.00956907, 0.00741606, 0.00708179, 0.00630002,\n",
      "       0.00662408, 0.00892797, 0.00780973, 0.00736094, 0.00909071,\n",
      "       0.00826235, 0.00711508, 0.00687432, 0.00662985, 0.00775924,\n",
      "       0.0071846 , 0.01340365, 0.00865803, 0.01018806, 0.00689983,\n",
      "       0.01309295, 0.00778008, 0.00703902, 0.00844579, 0.00589514]), 'std_score_time': array([0.00448065, 0.00054884, 0.0005567 , 0.00301625, 0.00041873,\n",
      "       0.00082962, 0.00139282, 0.0022603 , 0.00648041, 0.00066138,\n",
      "       0.00125738, 0.00070726, 0.00077382, 0.0022095 , 0.00637547,\n",
      "       0.00063078, 0.06495359, 0.00590984, 0.00069796, 0.00340982,\n",
      "       0.00447294, 0.00134655, 0.00066926, 0.00077153, 0.03306871,\n",
      "       0.00082667, 0.00260421, 0.00159485, 0.00159511, 0.00053665,\n",
      "       0.00063619, 0.00049839, 0.00065857, 0.00035875, 0.00076906,\n",
      "       0.0010797 , 0.00155146, 0.00121257, 0.00171995, 0.0016837 ,\n",
      "       0.00101561, 0.00151348, 0.00115701, 0.00064761, 0.00050092,\n",
      "       0.00134595, 0.00092049, 0.00322719, 0.0011964 , 0.00015352,\n",
      "       0.00200173, 0.00066926, 0.00076535, 0.00041807, 0.0011018 ,\n",
      "       0.00111587, 0.0033991 , 0.0015343 , 0.00044792, 0.00700897,\n",
      "       0.00381184, 0.01772907, 0.00915149, 0.0009319 , 0.0034678 ,\n",
      "       0.00097104, 0.00073017, 0.00153278, 0.00359251, 0.00061801,\n",
      "       0.00208534, 0.00090666, 0.07710507, 0.0014457 , 0.00231848,\n",
      "       0.00340345, 0.00429371, 0.0012253 , 0.00110429, 0.00044833,\n",
      "       0.0008744 , 0.00163866, 0.00075474, 0.00145018, 0.0019613 ,\n",
      "       0.00083438, 0.0013885 , 0.00128267, 0.00101946, 0.00319075,\n",
      "       0.00316097, 0.00716714, 0.00198986, 0.00235045, 0.00058163,\n",
      "       0.00834089, 0.00108456, 0.00095958, 0.00162054, 0.00072736]), 'param_subsample': masked_array(data=[0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8, 0.8, 0.6,\n",
      "                   0.8, 0.6, 0.8, 0.6, 1.0, 1.0, 0.8, 1.0, 0.6, 1.0, 0.6,\n",
      "                   0.6, 0.6, 1.0, 0.8, 0.6, 0.6, 1.0, 1.0, 1.0, 1.0, 0.8,\n",
      "                   0.6, 1.0, 0.8, 0.8, 0.6, 0.6, 0.6, 1.0, 1.0, 1.0, 0.6,\n",
      "                   0.6, 1.0, 0.6, 0.6, 0.8, 0.6, 0.6, 1.0, 0.8, 1.0, 1.0,\n",
      "                   0.8, 0.8, 0.8, 0.6, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
      "                   0.8, 0.6, 0.6, 0.8, 0.8, 1.0, 0.8, 0.6, 1.0, 0.6, 0.8,\n",
      "                   0.8, 0.6, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6, 0.8, 1.0,\n",
      "                   0.8, 1.0, 0.8, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.8, 0.8,\n",
      "                   0.6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_reg_alpha': masked_array(data=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_random_state': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[100, 300, 300, 100, 100, 300, 200, 100, 300, 100, 100,\n",
      "                   200, 200, 300, 300, 300, 200, 200, 300, 100, 200, 300,\n",
      "                   100, 100, 200, 200, 300, 200, 300, 300, 100, 300, 300,\n",
      "                   300, 200, 100, 300, 300, 200, 100, 300, 200, 100, 200,\n",
      "                   100, 200, 300, 100, 100, 100, 100, 100, 100, 200, 300,\n",
      "                   300, 200, 300, 200, 300, 100, 100, 300, 100, 200, 300,\n",
      "                   100, 300, 100, 200, 200, 200, 100, 300, 100, 300, 300,\n",
      "                   200, 300, 200, 100, 200, 300, 200, 200, 300, 100, 200,\n",
      "                   200, 100, 100, 200, 200, 300, 100, 100, 300, 300, 300,\n",
      "                   300],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[10, 5, 5, 10, 10, 10, 2, 2, 1, 1, 5, 2, 5, 5, 5, 10, 2,\n",
      "                   1, 2, 1, 1, 1, 10, 1, 10, 1, 2, 2, 10, 2, 10, 1, 1, 5,\n",
      "                   1, 1, 1, 1, 2, 1, 2, 5, 10, 2, 5, 5, 10, 10, 5, 5, 5,\n",
      "                   10, 5, 5, 5, 5, 5, 5, 5, 2, 10, 5, 2, 10, 5, 5, 5, 10,\n",
      "                   10, 5, 5, 10, 10, 1, 10, 1, 5, 1, 2, 5, 10, 10, 2, 2,\n",
      "                   5, 2, 2, 2, 2, 5, 1, 10, 10, 1, 2, 2, 2, 10, 1, 10],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[3, 4, 4, 5, 3, 4, 5, 5, 5, 4, 5, 5, 3, 4, 3, 3, 5, 3,\n",
      "                   4, 4, 4, 4, 3, 3, 4, 3, 5, 5, 5, 3, 3, 4, 3, 4, 4, 4,\n",
      "                   4, 5, 3, 5, 5, 4, 5, 4, 3, 5, 4, 5, 3, 5, 3, 4, 5, 3,\n",
      "                   3, 3, 4, 4, 4, 5, 4, 5, 3, 3, 3, 5, 5, 5, 4, 3, 3, 5,\n",
      "                   4, 4, 3, 3, 3, 5, 4, 4, 5, 3, 3, 5, 4, 3, 5, 3, 3, 4,\n",
      "                   3, 5, 3, 4, 4, 5, 4, 4, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_learning_rate': masked_array(data=[0.05, 0.05, 0.5, 0.2, 1, 0.2, 0.2, 0.01, 0.02, 0.1, 1,\n",
      "                   0.02, 1, 0.02, 0.02, 0.5, 0.01, 0.2, 0.01, 0.02, 0.2,\n",
      "                   1, 1, 0.1, 0.02, 0.05, 0.01, 0.01, 0.2, 0.2, 1, 0.5, 1,\n",
      "                   0.05, 1, 0.02, 0.02, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5,\n",
      "                   0.02, 0.2, 0.2, 1, 1, 0.2, 0.5, 1, 0.02, 0.5, 0.2,\n",
      "                   0.01, 0.02, 0.01, 0.2, 0.1, 0.01, 0.5, 1, 0.05, 0.5,\n",
      "                   0.01, 0.1, 0.1, 0.01, 0.1, 0.2, 0.05, 0.1, 0.1, 0.2, 1,\n",
      "                   0.02, 0.02, 0.2, 0.05, 0.1, 0.1, 0.1, 0.01, 0.5, 0.05,\n",
      "                   0.2, 0.01, 1, 0.01, 0.01, 0.5, 1, 0.01, 0.05, 0.1,\n",
      "                   0.05, 0.01, 0.2, 0.01, 0.01],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[5, 1, 5, 5, 5, 5, 0, 1, 1, 5, 5, 2, 2, 5, 0, 5, 2, 5,\n",
      "                   1, 5, 5, 2, 5, 2, 2, 1, 2, 0, 5, 5, 2, 0, 5, 5, 5, 0,\n",
      "                   2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 5, 1, 0, 0,\n",
      "                   0, 0, 1, 2, 0, 5, 0, 5, 5, 5, 2, 1, 2, 0, 2, 1, 0, 5,\n",
      "                   2, 0, 5, 5, 5, 0, 2, 5, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "                   5, 5, 5, 0, 0, 0, 0, 5, 0, 2],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_colsample_bytree': masked_array(data=[0.8, 0.8, 0.8, 0.6, 1.0, 0.8, 0.8, 0.6, 0.8, 1.0, 0.6,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.6, 1.0, 1.0, 0.6, 0.6, 1.0, 0.6,\n",
      "                   0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 1.0, 0.8,\n",
      "                   0.8, 0.8, 0.6, 0.8, 0.8, 0.6, 0.8, 0.6, 0.6, 0.8, 0.8,\n",
      "                   1.0, 1.0, 0.6, 0.8, 1.0, 0.6, 1.0, 0.6, 1.0, 0.6, 0.8,\n",
      "                   1.0, 0.6, 1.0, 0.8, 0.8, 0.8, 0.6, 1.0, 0.6, 1.0, 0.8,\n",
      "                   0.6, 0.6, 0.8, 1.0, 1.0, 0.6, 1.0, 0.6, 1.0, 0.8, 0.6,\n",
      "                   1.0, 0.8, 1.0, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 1.0, 0.8,\n",
      "                   0.6, 0.6, 0.6, 0.8, 0.6, 0.8, 0.8, 1.0, 0.8, 0.6, 0.8,\n",
      "                   0.8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.5, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.02, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.02, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 1, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.02, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 0.5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 1, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 1, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.5, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.5, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.5, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 1, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 1, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.5, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.02, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.5, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.02, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 2, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.5, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 0.5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 2, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 2, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.02, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.02, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.5, 'gamma': 0, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 1, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 2, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 1, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 1, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 200, 'min_child_weight': 10, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 4, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 100, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 4, 'learning_rate': 0.2, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.8}, {'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 10, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 0.8}], 'split0_test_score': array([0.90140845, 0.89295775, 0.87605634, 0.89295775, 0.86478873,\n",
      "       0.89014085, 0.88169014, 0.86760563, 0.88732394, 0.88450704,\n",
      "       0.83661972, 0.89014085, 0.78591549, 0.89859155, 0.89859155,\n",
      "       0.88732394, 0.87605634, 0.88732394, 0.89295775, 0.89859155,\n",
      "       0.88732394, 0.83661972, 0.82816901, 0.88732394, 0.89577465,\n",
      "       0.89295775, 0.89859155, 0.89859155, 0.89859155, 0.90140845,\n",
      "       0.84507042, 0.88732394, 0.82816901, 0.89859155, 0.84507042,\n",
      "       0.89014085, 0.89014085, 0.89295775, 0.88450704, 0.89295775,\n",
      "       0.89859155, 0.89295775, 0.88732394, 0.90140845, 0.89014085,\n",
      "       0.89295775, 0.81690141, 0.82816901, 0.89014085, 0.86197183,\n",
      "       0.83380282, 0.88732394, 0.87605634, 0.88169014, 0.89295775,\n",
      "       0.90140845, 0.89295775, 0.90422535, 0.89859155, 0.90140845,\n",
      "       0.87605634, 0.84788732, 0.89577465, 0.88732394, 0.89014085,\n",
      "       0.89295775, 0.89859155, 0.89577465, 0.88450704, 0.89577465,\n",
      "       0.89859155, 0.89014085, 0.88732394, 0.87887324, 0.84225352,\n",
      "       0.90140845, 0.90704225, 0.88169014, 0.89295775, 0.89295775,\n",
      "       0.89859155, 0.89014085, 0.89577465, 0.88169014, 0.89014085,\n",
      "       0.88169014, 0.85070423, 0.86760563, 0.89577465, 0.86760563,\n",
      "       0.89014085, 0.83380282, 0.89295775, 0.88169014, 0.89295775,\n",
      "       0.88169014, 0.89295775, 0.89295775, 0.89859155, 0.89859155]), 'split1_test_score': array([0.89548023, 0.89548023, 0.88983051, 0.89548023, 0.8559322 ,\n",
      "       0.89830508, 0.88135593, 0.88700565, 0.89830508, 0.90960452,\n",
      "       0.8559322 , 0.89830508, 0.85310734, 0.90112994, 0.9039548 ,\n",
      "       0.88418079, 0.89265537, 0.89548023, 0.89830508, 0.91242938,\n",
      "       0.90960452, 0.85875706, 0.84745763, 0.90112994, 0.89830508,\n",
      "       0.90112994, 0.90112994, 0.90112994, 0.9039548 , 0.90112994,\n",
      "       0.8559322 , 0.88700565, 0.8700565 , 0.90112994, 0.86723164,\n",
      "       0.90677966, 0.89830508, 0.88418079, 0.88418079, 0.88983051,\n",
      "       0.88135593, 0.88135593, 0.89265537, 0.90677966, 0.9039548 ,\n",
      "       0.87288136, 0.84745763, 0.85310734, 0.88983051, 0.86723164,\n",
      "       0.86723164, 0.9039548 , 0.88700565, 0.88135593, 0.90960452,\n",
      "       0.89548023, 0.90112994, 0.88983051, 0.88418079, 0.90677966,\n",
      "       0.88418079, 0.83898305, 0.90677966, 0.88418079, 0.89830508,\n",
      "       0.87288136, 0.88135593, 0.9039548 , 0.89265537, 0.88983051,\n",
      "       0.89265537, 0.90677966, 0.9039548 , 0.88135593, 0.89265537,\n",
      "       0.9039548 , 0.90960452, 0.88983051, 0.89830508, 0.89265537,\n",
      "       0.89548023, 0.89830508, 0.91242938, 0.88418079, 0.88135593,\n",
      "       0.87853107, 0.87570621, 0.8700565 , 0.9039548 , 0.9039548 ,\n",
      "       0.90112994, 0.86158192, 0.90112994, 0.87853107, 0.89548023,\n",
      "       0.88983051, 0.90112994, 0.90677966, 0.90112994, 0.9039548 ]), 'split2_test_score': array([0.87853107, 0.88418079, 0.87570621, 0.87288136, 0.85875706,\n",
      "       0.88418079, 0.89548023, 0.86440678, 0.89265537, 0.88700565,\n",
      "       0.86440678, 0.88983051, 0.85875706, 0.87570621, 0.87853107,\n",
      "       0.88418079, 0.87570621, 0.88135593, 0.88700565, 0.87570621,\n",
      "       0.87570621, 0.8220339 , 0.84463277, 0.88700565, 0.8700565 ,\n",
      "       0.88700565, 0.88983051, 0.88700565, 0.87570621, 0.87570621,\n",
      "       0.8700565 , 0.88418079, 0.8559322 , 0.88418079, 0.84745763,\n",
      "       0.88700565, 0.88418079, 0.88983051, 0.89830508, 0.88135593,\n",
      "       0.88418079, 0.8700565 , 0.88700565, 0.88135593, 0.87853107,\n",
      "       0.87853107, 0.83615819, 0.85310734, 0.87570621, 0.87570621,\n",
      "       0.83898305, 0.8700565 , 0.87288136, 0.87570621, 0.87288136,\n",
      "       0.87853107, 0.87570621, 0.88135593, 0.88418079, 0.87570621,\n",
      "       0.8700565 , 0.83050847, 0.88418079, 0.87288136, 0.86440678,\n",
      "       0.87288136, 0.89548023, 0.87288136, 0.88135593, 0.87570621,\n",
      "       0.88135593, 0.87853107, 0.88700565, 0.88983051, 0.85875706,\n",
      "       0.88135593, 0.87853107, 0.88700565, 0.88983051, 0.88135593,\n",
      "       0.88983051, 0.86440678, 0.88135593, 0.88418079, 0.88135593,\n",
      "       0.87853107, 0.85028249, 0.86723164, 0.88135593, 0.85875706,\n",
      "       0.85310734, 0.85310734, 0.86723164, 0.88418079, 0.88135593,\n",
      "       0.88135593, 0.87570621, 0.87288136, 0.88983051, 0.86440678]), 'split3_test_score': array([0.88135593, 0.87570621, 0.8700565 , 0.87288136, 0.85028249,\n",
      "       0.87288136, 0.87853107, 0.87288136, 0.87853107, 0.8700565 ,\n",
      "       0.81920904, 0.88700565, 0.82768362, 0.88418079, 0.88135593,\n",
      "       0.87853107, 0.8700565 , 0.88700565, 0.87853107, 0.87853107,\n",
      "       0.87288136, 0.85028249, 0.82768362, 0.88418079, 0.87570621,\n",
      "       0.88700565, 0.88418079, 0.87853107, 0.87288136, 0.88135593,\n",
      "       0.86440678, 0.86158192, 0.85875706, 0.88983051, 0.86158192,\n",
      "       0.88135593, 0.88418079, 0.87570621, 0.88418079, 0.87570621,\n",
      "       0.88700565, 0.86158192, 0.88418079, 0.88135593, 0.85875706,\n",
      "       0.8700565 , 0.86440678, 0.8220339 , 0.8700565 , 0.86723164,\n",
      "       0.81638418, 0.87288136, 0.87570621, 0.87570621, 0.87570621,\n",
      "       0.87570621, 0.88135593, 0.85875706, 0.87570621, 0.87288136,\n",
      "       0.88418079, 0.81073446, 0.88418079, 0.88700565, 0.88418079,\n",
      "       0.87853107, 0.88418079, 0.88135593, 0.88135593, 0.87288136,\n",
      "       0.88135593, 0.88418079, 0.87570621, 0.86440678, 0.86723164,\n",
      "       0.88700565, 0.88418079, 0.88135593, 0.87288136, 0.87853107,\n",
      "       0.8700565 , 0.87570621, 0.88135593, 0.87570621, 0.88135593,\n",
      "       0.85310734, 0.87853107, 0.86158192, 0.87853107, 0.88418079,\n",
      "       0.87853107, 0.88983051, 0.8700565 , 0.87570621, 0.88700565,\n",
      "       0.88135593, 0.88135593, 0.88135593, 0.88700565, 0.88418079]), 'split4_test_score': array([0.87570621, 0.86723164, 0.8559322 , 0.8700565 , 0.84745763,\n",
      "       0.86440678, 0.8700565 , 0.84463277, 0.88418079, 0.87853107,\n",
      "       0.84745763, 0.88135593, 0.86440678, 0.87853107, 0.87853107,\n",
      "       0.85875706, 0.86440678, 0.86723164, 0.88700565, 0.87853107,\n",
      "       0.8700565 , 0.86158192, 0.85310734, 0.86723164, 0.87853107,\n",
      "       0.87570621, 0.88418079, 0.8700565 , 0.86440678, 0.8700565 ,\n",
      "       0.85028249, 0.87288136, 0.83898305, 0.87288136, 0.8559322 ,\n",
      "       0.87853107, 0.88418079, 0.86723164, 0.86723164, 0.86723164,\n",
      "       0.8700565 , 0.87288136, 0.8700565 , 0.87570621, 0.86723164,\n",
      "       0.87288136, 0.82485876, 0.84463277, 0.86723164, 0.84463277,\n",
      "       0.86158192, 0.8700565 , 0.86440678, 0.87288136, 0.87570621,\n",
      "       0.87853107, 0.88418079, 0.87570621, 0.86440678, 0.87853107,\n",
      "       0.85875706, 0.84180791, 0.87570621, 0.85875706, 0.8700565 ,\n",
      "       0.86440678, 0.8559322 , 0.88700565, 0.87288136, 0.86158192,\n",
      "       0.8700565 , 0.87853107, 0.8700565 , 0.8700565 , 0.87853107,\n",
      "       0.87853107, 0.87853107, 0.88135593, 0.87288136, 0.87570621,\n",
      "       0.86440678, 0.86440678, 0.88135593, 0.8559322 , 0.87570621,\n",
      "       0.86158192, 0.84745763, 0.84745763, 0.88418079, 0.84463277,\n",
      "       0.85310734, 0.85028249, 0.87288136, 0.87288136, 0.8700565 ,\n",
      "       0.87570621, 0.87853107, 0.86158192, 0.87570621, 0.87853107]), 'mean_test_score': array([0.88649638, 0.88311132, 0.87351635, 0.88085144, 0.85544362,\n",
      "       0.88198297, 0.88142277, 0.86730644, 0.88819925, 0.88594096,\n",
      "       0.84472507, 0.8893276 , 0.83797406, 0.88762791, 0.88819289,\n",
      "       0.87859473, 0.87577624, 0.88367948, 0.88876104, 0.88875786,\n",
      "       0.88311451, 0.84585502, 0.84021007, 0.88537439, 0.8836747 ,\n",
      "       0.88876104, 0.89158272, 0.88706294, 0.88310814, 0.88593141,\n",
      "       0.85714968, 0.87859473, 0.85037957, 0.88932283, 0.85545476,\n",
      "       0.88876263, 0.88819766, 0.88198138, 0.88368107, 0.88141641,\n",
      "       0.88423808, 0.87576669, 0.88424445, 0.88932124, 0.87972308,\n",
      "       0.87746161, 0.83795655, 0.84021007, 0.87859314, 0.86335482,\n",
      "       0.84359672, 0.88085462, 0.87521127, 0.87746797, 0.88537121,\n",
      "       0.88593141, 0.88706613, 0.88197501, 0.88141323, 0.88706135,\n",
      "       0.8746463 , 0.83398424, 0.88932442, 0.87802976, 0.881418  ,\n",
      "       0.87633166, 0.88310814, 0.88819448, 0.88255113, 0.87915493,\n",
      "       0.88480306, 0.88763269, 0.88480942, 0.87690459, 0.86788573,\n",
      "       0.89045118, 0.89157794, 0.88424763, 0.88537121, 0.88424127,\n",
      "       0.88367311, 0.87859314, 0.89045436, 0.87633803, 0.88198297,\n",
      "       0.87068831, 0.86053633, 0.86278666, 0.88875945, 0.87182621,\n",
      "       0.87520331, 0.85772102, 0.88085144, 0.87859792, 0.88537121,\n",
      "       0.88198775, 0.88593618, 0.88311132, 0.89045277, 0.885933  ]), 'std_test_score': array([0.01009334, 0.0105649 , 0.0109414 , 0.01099217, 0.00614755,\n",
      "       0.01207936, 0.00818831, 0.01372393, 0.00681722, 0.01319376,\n",
      "       0.01573023, 0.00548541, 0.02889853, 0.01038526, 0.01086292,\n",
      "       0.01031644, 0.00945482, 0.00937586, 0.00662595, 0.01439835,\n",
      "       0.01448828, 0.01473864, 0.01039544, 0.01081927, 0.01127711,\n",
      "       0.00833299, 0.00711225, 0.01177119, 0.01538464, 0.01302334,\n",
      "       0.00909726, 0.01000225, 0.01490896, 0.01022093, 0.0083457 ,\n",
      "       0.00989084, 0.00555593, 0.00942188, 0.00985522, 0.00935753,\n",
      "       0.00919901, 0.01066688, 0.00760464, 0.01235442, 0.01607941,\n",
      "       0.00822274, 0.01678088, 0.01286546, 0.00969454, 0.01034451,\n",
      "       0.01865372, 0.01320789, 0.00724199, 0.00346951, 0.01405439,\n",
      "       0.01043842, 0.00897197, 0.01507011, 0.01124458, 0.01412394,\n",
      "       0.00956384, 0.01290059, 0.01081351, 0.01097931, 0.01254879,\n",
      "       0.00945808, 0.01507021, 0.01084912, 0.00636055, 0.01224945,\n",
      "       0.00992983, 0.01049278, 0.01164654, 0.00887537, 0.01713259,\n",
      "       0.01038358, 0.01385108, 0.00352249, 0.01055196, 0.00721876,\n",
      "       0.01383102, 0.01365833, 0.01232519, 0.01066264, 0.00462878,\n",
      "       0.01127896, 0.0136147 , 0.00815183, 0.00959695, 0.02055715,\n",
      "       0.01940545, 0.01840991, 0.0135892 , 0.00404415, 0.00908936,\n",
      "       0.00451339, 0.00959812, 0.01567413, 0.00905508, 0.01418634]), 'rank_test_score': array([ 24,  45,  80,  60,  91,  51,  55,  84,  15,  25,  94,   6,  98,\n",
      "        20,  18,  65,  75,  41,  11,  14,  44,  93,  97,  30,  42,  11,\n",
      "         1,  22,  47,  28,  89,  65,  92,   8,  90,  10,  16,  53,  40,\n",
      "        57,  39,  76,  37,   9,  62,  71,  99,  96,  67,  85,  95,  59,\n",
      "        77,  70,  31,  28,  21,  54,  58,  23,  79, 100,   7,  69,  56,\n",
      "        74,  47,  17,  49,  63,  35,  19,  34,  72,  83,   5,   2,  36,\n",
      "        31,  38,  43,  67,   3,  73,  51,  82,  87,  86,  13,  81,  78,\n",
      "        88,  60,  64,  31,  50,  26,  45,   4,  27], dtype=int32)}\n",
      "\n",
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.7831733483907397, booster='gbtree',\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
      "              gamma=2, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
      "              max_depth=5, min_child_weight=2, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=300, n_jobs=4, nthread=-1,\n",
      "              num_parallel_tree=1, random_state=1, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=1, subsample=0.6, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "\n",
      "\n",
      " Best accuracy 5-fold search with 100 parameter combinations:\n",
      "-0.8915827166388158\n",
      "\n",
      "\n",
      " Best accuracy 5-fold search with 100 parameter combinations:\n",
      "0.8754208754208754\n",
      "\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search1.cv_results_)\n",
    "print('\\n\\n Best estimator:')\n",
    "print(random_search1.best_estimator_)\n",
    "print('\\n\\n Best accuracy %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(-random_search1.best_score_)\n",
    "print('\\n\\n Best accuracy %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(accuracy_score(y_WSin_2019, random_search1.predict(X_famd_2019)))\n",
    "print('\\n\\n Best hyperparameters:')\n",
    "print(random_search1.best_params_)\n",
    "results1 = pd.DataFrame(random_search1.cv_results_).sort_values('rank_test_score', ascending=True)\n",
    "results1.to_csv('XGBoost/xgb-random-grid-search-results-train_WS64_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exotic-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best Train Accuracy 5-fold search with 100 parameter combinations:\n",
      "0.9311123658949746\n",
      "\n",
      "\n",
      "[[1377   10]\n",
      " [ 112  272]]\n",
      "\n",
      "\n",
      "Best Test Accuracy 5-fold search with 100 parameter combinations:\n",
      "0.8754208754208754\n",
      "\n",
      "\n",
      "[[225   8]\n",
      " [ 29  35]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nBest Train Accuracy %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(accuracy_score(y_WSin, random_search1.predict(X_famd)))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_WSin,random_search1.predict(X_famd)))\n",
    "\n",
    "\n",
    "print('\\n\\nBest Test Accuracy %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(accuracy_score(y_WSin_2019, random_search1.predict(X_famd_2019)))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_WSin_2019,random_search1.predict(X_famd_2019)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "identified-travel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>ImportanceGain</th>\n",
       "      <th>CummulativeGain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.104826</td>\n",
       "      <td>0.104826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.043215</td>\n",
       "      <td>0.148041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.186044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.034953</td>\n",
       "      <td>0.220997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>0.251111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.279433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.024916</td>\n",
       "      <td>0.304349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.328507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.352323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.023417</td>\n",
       "      <td>0.375740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.398648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.021980</td>\n",
       "      <td>0.420628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.442556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.463539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.020974</td>\n",
       "      <td>0.484513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>0.504457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.019447</td>\n",
       "      <td>0.523904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.018943</td>\n",
       "      <td>0.542847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.561280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>0.579619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.597317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.614768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>0.631875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.017039</td>\n",
       "      <td>0.648914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>0.665865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.682677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.016680</td>\n",
       "      <td>0.699357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.715949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.732490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>0.764899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>0.780948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>0.796994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.812924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.828728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.843789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>0.858721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.873624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.888458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.014829</td>\n",
       "      <td>0.903287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.917970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.932635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.947268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.014005</td>\n",
       "      <td>0.961273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>0.974825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>0.987903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Features  ImportanceGain  CummulativeGain\n",
       "0         0        0.104826         0.104826\n",
       "18       18        0.043215         0.148041\n",
       "6         6        0.038002         0.186044\n",
       "8         8        0.034953         0.220997\n",
       "29       29        0.030114         0.251111\n",
       "27       27        0.028322         0.279433\n",
       "15       15        0.024916         0.304349\n",
       "7         7        0.024158         0.328507\n",
       "1         1        0.023816         0.352323\n",
       "11       11        0.023417         0.375740\n",
       "16       16        0.022909         0.398648\n",
       "25       25        0.021980         0.420628\n",
       "12       12        0.021928         0.442556\n",
       "43       43        0.020982         0.463539\n",
       "13       13        0.020974         0.484513\n",
       "10       10        0.019944         0.504457\n",
       "45       45        0.019447         0.523904\n",
       "31       31        0.018943         0.542847\n",
       "22       22        0.018433         0.561280\n",
       "42       42        0.018339         0.579619\n",
       "32       32        0.017698         0.597317\n",
       "23       23        0.017452         0.614768\n",
       "33       33        0.017107         0.631875\n",
       "19       19        0.017039         0.648914\n",
       "17       17        0.016952         0.665865\n",
       "26       26        0.016812         0.682677\n",
       "14       14        0.016680         0.699357\n",
       "5         5        0.016592         0.715949\n",
       "40       40        0.016541         0.732490\n",
       "39       39        0.016210         0.748700\n",
       "20       20        0.016199         0.764899\n",
       "36       36        0.016049         0.780948\n",
       "37       37        0.016046         0.796994\n",
       "35       35        0.015929         0.812924\n",
       "38       38        0.015805         0.828728\n",
       "2         2        0.015061         0.843789\n",
       "21       21        0.014932         0.858721\n",
       "34       34        0.014903         0.873624\n",
       "44       44        0.014834         0.888458\n",
       "28       28        0.014829         0.903287\n",
       "46       46        0.014684         0.917970\n",
       "9         9        0.014664         0.932635\n",
       "30       30        0.014634         0.947268\n",
       "4         4        0.014005         0.961273\n",
       "41       41        0.013553         0.974825\n",
       "3         3        0.013078         0.987903\n",
       "24       24        0.012097         1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance1 = pd.DataFrame(X_famd.columns)\n",
    "feature_importance1.columns = ['Features']\n",
    "feature_importance1['ImportanceGain'] = random_search1.best_estimator_.feature_importances_\n",
    "feature_importance1 = feature_importance1.sort_values('ImportanceGain', ascending=False)\n",
    "feature_importance1['CummulativeGain'] = feature_importance1.ImportanceGain.cumsum()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "feature_importance1 = feature_importance1.sort_values('ImportanceGain', ascending=False)\n",
    "feature_importance1.sort_values('ImportanceGain', ascending=True).plot(x='Features',y='ImportanceGain',kind='barh')\n",
    "plt.show()\n",
    "\n",
    "feature_importance1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "liquid-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.7831733483907397, booster='gbtree',\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.6,\n",
      "              gamma=2, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
      "              max_depth=5, min_child_weight=2, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=300, n_jobs=4, nthread=-1,\n",
      "              num_parallel_tree=1, random_state=1, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=1, subsample=0.6, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None)\n",
      "{'subsample': 0.6, 'reg_alpha': 0, 'random_state': 1, 'n_estimators': 300, 'min_child_weight': 2, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 2, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print(random_search1.best_estimator_)\n",
    "print(random_search1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "conscious-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:43:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "best_xgbmodel = random_search1.best_estimator_\n",
    "best_xgbmodel.fit(X_famd,y_WSin,sample_weight = None)\n",
    "fname = 'WS64Seeds/xgbmodel_WSin.obj'\n",
    "file = open(fname,'wb')\n",
    "pickle.dump(best_xgbmodel,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-earth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:44:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:32] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:35] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:37] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:38] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:39] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:43] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:48] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:52] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:57] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:58] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:44:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:00] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:04] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:06] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:11] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:17] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:19] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:22] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:29] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# boostrap resampling to generate prediction probability\n",
    "n_bootstrap = 100\n",
    "best_xgbmodel_i = random_search1.best_estimator_\n",
    "\n",
    "y_ex = y_WSin_2019\n",
    "X_ex = pd.DataFrame(X_famd_2019)\n",
    "df_y_ex = pd.DataFrame()\n",
    "df_prob_in = pd.DataFrame()\n",
    "scores = []\n",
    "for i_b in range(n_bootstrap):\n",
    "    idx_resample =  resample(range(len(y_WSin)), replace=True, n_samples=len(y_WSin))\n",
    "    X_i = X_famd.iloc[idx_resample]\n",
    "    y_i = y_WSin.iloc[idx_resample]\n",
    "    best_xgbmodel_i.fit(X_i,y_i,sample_weight = None)\n",
    "    idx_test = y_WSin.index[~y_WSin.index.isin(y_i.index)]\n",
    "    scores.append(best_xgbmodel_i.score(X_famd.loc[idx_test],y_WSin.loc[idx_test]))\n",
    "    \n",
    "    y_ex_est = pd.Series(best_xgbmodel_i.predict(X_ex))\n",
    "    df_y_ex = df_y_ex.append(y_ex_est,ignore_index = True)\n",
    "    prob_in_i = pd.DataFrame(best_xgbmodel_i.predict_proba(X_famd_2019)[:,1].reshape([1,-1])).round(decimals=2)\n",
    "    df_prob_in = df_prob_in.append(prob_in_i,ignore_index = True)\n",
    "\n",
    "df_y_ex.columns = X_College_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(scores,decimals=3))\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob_in.to_csv('WS64Seeds/WS64_prediction_logistic.csv',index = False)\n",
    "df_prob_in_5 = (df_prob_in*100/5).round(0)*5\n",
    "df_prob_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ex = 30\n",
    "college = df_prob_in.columns[idx_ex]\n",
    "bins = np.linspace(0,1,20)\n",
    "sn.displot(data=df_prob_in,x=college,kde=False,bins=bins) \n",
    "plt.title(college + ' WS Seeding % Chance')\n",
    "plt.ylabel('Percent')\n",
    "plt.show()\n",
    "print('Actual result: ', y_WSin_2019.WCWS_in.loc[idx_ex])\n",
    "print('Predicted result (% peak chance making into WS): ',df_prob_in_5[college].value_counts().idxmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
